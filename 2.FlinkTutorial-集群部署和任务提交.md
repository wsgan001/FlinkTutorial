### 集群部署和任务提交

---
#### Standalone模式

- 解压缩安装包
- 配置文件的配置
    - conf/flink-yaml
        - jobmanager.rpc.address: localhost
        - jobmanager.heap.size: 1024m
        - taskmanager.memory.process.size: 1024m
        - taskmanager.numberOfTaskSlots: 2
        - parallelism.default: 1
    - conf/masters
        - 配置jobmanager
    - conf/slaves
        - 配置taskmanager的机器
- 启动集群
    - bin/start-cluster.sh
        - standalonesession daemon
        - taskexecutor daemon

- 停止集群
    - bin/stop-cluster.sh

#### 提交任务
- 在Flink UI上提交打包好的任务
    - 指定入口类的全限定名
    - 需要指定的其他参数
    - 并行度
    - 检查点路径

- 在Flink命令行中提交任务
    - bin/flink run -c MainFullPath.class -p 1 jar/path/xx.jar arguments
    - bin/flink cancel jobId
    - bin/flink list 列出当前进行的job

- 指定并行度的几种方式(优先级从高到低)
    - 算子上指定
    - StreamingExecutionEnvironment.setParallelism()
    - 提交任务时指定
    - flink-yaml中指定

#### Yarn模式
要求
- Flink有Hadoop支持的版本(lib/flink-shaded-hadoop-2-uber-2.7.5-10.0.jar)
- Hadoop环境需要在2.2以上
- 集群中安装HDFS服务

Flink On Yarn
- Session-Cluster模式()

    需要先启动集群，然后提交作业，接着向Yarn申请一块空间，资源保持不变，如果资源满了，则无法提交新作业。所有作业共享Dispatcher和ResourceManager，适合规模小执行时间短的作业
    - bin/yarn-session.sh -n 2 -s 2 -jm 1024 -tm 1024 -nm test -d
    - -n(--container) TaskManager的数量，可以不指定
    - -s(--slots) 每个TaskManager的slot数量，默认一个slot一个core
    - -jm 每个JobManager的内存(MB)
    - -tm 每个TaskManager的内存(MB)
    - -nm yarn的appName
    - -d后台执行
    - 执行任务和在Flink命令行中提交任务一样
    - 查看任务状态可以到Yarn的控制台里查看
    - 取消yarn-session
        - yarn application --kill applicationId
- Per-Job模式

    一个Job对应一个集群，每提交一个作业会根据自身的情况，单独向Yarn申请资源，直到作业执行完成，一个作业的失败与否不会影响下一个作业的正常提交和运行。独享Dispatcher和ResourceManager，适合大规模长时间运行的作业
    - 启动Hadoop集群
    - bin/flink run `-m yarn-cluster` -c MainFullPath.class -p 1 jar/path/xx.jar arguments 
